# ğŸš€ NoCode AI Agent - Workflow Builder

A powerful no-code platform for building AI-powered document processing workflows with a visual node-based interface. Create sophisticated RAG (Retrieval-Augmented Generation) pipelines without writing code!

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Architecture](#architecture)
- [Prerequisites](#prerequisites)
- [Quick Start](#quick-start)
- [Backend Setup](#backend-setup)
- [Frontend Setup](#frontend-setup)
- [Usage Guide](#usage-guide)
- [API Documentation](#api-documentation)
- [Project Structure](#project-structure)
- [Troubleshooting](#troubleshooting)
- [Contributing](#contributing)

## ğŸŒŸ Overview

NoCode AI Agent is a full-stack application that enables users to:
- Build visual AI workflows using drag-and-drop nodes
- Upload and process PDF documents
- Query documents using state-of-the-art LLMs (GPT-4, Gemini)
- Get accurate, context-aware answers with source citations
- Ask follow-up questions with conversation memory
- Leverage vector search for semantic document retrieval

## âœ¨ Features

### Frontend
- ğŸ¨ **Visual Workflow Builder** - Drag-and-drop node-based interface
- ğŸ“„ **Document Upload** - Direct PDF upload with progress tracking
- ğŸ¤– **Multi-Provider Support** - OpenAI and Google Gemini integration
- ğŸ’¬ **Interactive Chat** - Ask questions and get AI-powered answers
- ğŸ”„ **Follow-up Questions** - Continue conversations with context
- ğŸ“Š **Confidence Scoring** - Visual feedback on answer reliability
- ğŸ¯ **Custom Prompts** - Override system behavior
- âš¡ **Real-time Updates** - Live workflow status and notifications

### Backend
- ğŸ” **Vector Search** - Qdrant integration for semantic similarity
- ğŸ“š **Document Processing** - Intelligent text extraction and chunking
- ğŸ§  **RAG Pipeline** - Retrieval-Augmented Generation
- ğŸ’¾ **PostgreSQL Database** - Document and chat history storage
- ğŸŒ **Web Search Fallback** - Google Search API integration
- ğŸ”Œ **RESTful API** - Well-documented FastAPI endpoints
- ğŸ“Š **Confidence Scoring** - Algorithmic response quality assessment
- ğŸ”„ **Background Jobs** - RQ (Redis Queue) for async processing

## ğŸ—ï¸ Architecture

### System Architecture Diagram

```mermaid
graph TB
    subgraph "Frontend - React + Vite"
        UI[User Interface]
        UserNode[User Input Node]
        KBNode[Knowledge Base Node]
        LLMNode[LLM Configuration Node]
        OutputNode[Output Display Node]
        
        UI --> UserNode
        UserNode --> KBNode
        KBNode --> LLMNode
        LLMNode --> OutputNode
    end
    
    subgraph "Backend - FastAPI"
        UploadAPI[Upload API<br/>/knowledge/upload]
        ProcessAPI[Process API<br/>/process/document]
        LLMAPI[LLM API<br/>/llm/process]
        OutputAPI[Output API<br/>/output/follow-up]
        
        Worker[RQ Background Worker]
    end
    
    subgraph "Data Layer"
        PostgreSQL[(PostgreSQL<br/>Document Metadata<br/>Chat Logs)]
        Qdrant[(Qdrant<br/>Vector Store<br/>Embeddings)]
        Redis[(Redis<br/>Job Queue)]
    end
    
    subgraph "External Services"
        OpenAI[OpenAI API<br/>GPT-4, Embeddings]
        Gemini[Google Gemini<br/>Gemini Pro]
        WebSearch[Google Search<br/>Web Fallback]
    end
    
    %% Frontend to Backend
    KBNode -->|Upload PDF| UploadAPI
    KBNode -->|Process Document| ProcessAPI
    LLMNode -->|Query + RAG| LLMAPI
    OutputNode -->|Follow-up| OutputAPI
    
    %% Backend to Data Layer
    UploadAPI -->|Save Metadata| PostgreSQL
    ProcessAPI -->|Queue Job| Redis
    LLMAPI -->|Store Chat| PostgreSQL
    LLMAPI -->|Search Vectors| Qdrant
    
    %% Worker Processing
    Redis -->|Consume Jobs| Worker
    Worker -->|Generate Embeddings| OpenAI
    Worker -->|Generate Embeddings| Gemini
    Worker -->|Store Vectors| Qdrant
    Worker -->|Update Status| PostgreSQL
    
    %% LLM Processing
    LLMAPI -->|Generate Response| OpenAI
    LLMAPI -->|Generate Response| Gemini
    LLMAPI -->|Fallback Search| WebSearch
    
    style UI fill:#e1f5ff
    style PostgreSQL fill:#336791
    style Qdrant fill:#DC244C
    style Redis fill:#DC382D
    style OpenAI fill:#10a37f
    style Gemini fill:#4285f4
```

### Data Flow Diagram

```mermaid
sequenceDiagram
    participant User
    participant Frontend
    participant Backend
    participant Worker
    participant Qdrant
    participant LLM
    participant DB
    
    %% Document Upload Flow
    User->>Frontend: Upload PDF
    Frontend->>Backend: POST /knowledge/upload
    Backend->>Backend: Extract Text & Chunk
    Backend->>DB: Save Document Metadata
    Backend->>Worker: Queue Embedding Job
    Backend-->>Frontend: Return document_id
    
    Worker->>LLM: Generate Embeddings
    LLM-->>Worker: Return Vectors
    Worker->>Qdrant: Store Vectors
    Worker->>DB: Update Status
    
    %% Query Flow
    User->>Frontend: Enter Query
    Frontend->>Backend: POST /llm/process
    Backend->>Qdrant: Vector Search (Top K)
    Qdrant-->>Backend: Return Relevant Chunks
    Backend->>LLM: RAG Prompt + Context
    LLM-->>Backend: Generated Answer
    Backend->>DB: Save Chat Log
    Backend-->>Frontend: Return Answer
    Frontend-->>User: Display Response
    
    %% Follow-up Flow
    User->>Frontend: Ask Follow-up
    Frontend->>Backend: POST /output/follow-up
    Backend->>DB: Retrieve Chat History
    Backend->>Qdrant: Vector Search
    Qdrant-->>Backend: Relevant Chunks
    Backend->>LLM: Context + History + Query
    LLM-->>Backend: Follow-up Answer
    Backend->>DB: Update Chat Log
    Backend-->>Frontend: Return Answer
    Frontend-->>User: Display Response
```

### Component Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FRONTEND (Port 5173)                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  React Flow Canvas (Zustand State Management)          â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚   User   â”‚ â”‚Knowledge â”‚ â”‚   LLM    â”‚ â”‚  Output  â”‚  â”‚    â”‚
â”‚  â”‚  â”‚  Input   â”‚â†’â”‚   Base   â”‚â†’â”‚   Node   â”‚â†’â”‚   Node   â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“ HTTP/REST API
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BACKEND (Port 8000)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  FastAPI Application (CORS Enabled)                    â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚  Upload  â”‚ â”‚ Process  â”‚ â”‚   LLM    â”‚ â”‚  Output  â”‚  â”‚    â”‚
â”‚  â”‚  â”‚  Routes  â”‚ â”‚  Routes  â”‚ â”‚  Routes  â”‚ â”‚  Routes  â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚          â”‚            â”‚            â”‚            â”‚              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚             Business Logic Layer                       â”‚    â”‚
â”‚  â”‚  â€¢ File Loader    â€¢ Text Extractor                     â”‚    â”‚
â”‚  â”‚  â€¢ Text Chunker   â€¢ Embedding Services                 â”‚    â”‚
â”‚  â”‚  â€¢ Vector Store   â€¢ Web Search                         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚              â”‚              â”‚              â”‚
         â–¼              â–¼              â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL  â”‚ â”‚    Qdrant    â”‚ â”‚  OpenAI/     â”‚ â”‚    Redis     â”‚
â”‚   Database   â”‚ â”‚ Vector Store â”‚ â”‚   Gemini     â”‚ â”‚    Queue     â”‚
â”‚              â”‚ â”‚              â”‚ â”‚     API      â”‚ â”‚              â”‚
â”‚ â€¢ Documents  â”‚ â”‚ â€¢ Embeddings â”‚ â”‚ â€¢ GPT-4      â”‚ â”‚ â€¢ Job Queue  â”‚
â”‚ â€¢ Chat Logs  â”‚ â”‚ â€¢ Collectionsâ”‚ â”‚ â€¢ Gemini Pro â”‚ â”‚ â€¢ RQ Worker  â”‚
â”‚ â€¢ Metadata   â”‚ â”‚ â€¢ Similarity â”‚ â”‚ â€¢ Embeddings â”‚ â”‚ â€¢ Async Jobs â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     Port 5432       Port 6333       External API      Port 6379
```

## ğŸ“‹ Prerequisites

### System Requirements
- **Node.js** 18+ and npm/yarn/pnpm
- **Python** 3.12+
- **PostgreSQL** 14+
- **Redis** (for background jobs)
- **Qdrant** (vector database)

### API Keys
- **OpenAI API Key** - For GPT models and embeddings
- **Google Gemini API Key** - For Gemini models (optional)
- **Google Search API** - For web search fallback (optional)

### Operating Systems
- âœ… Linux (Ubuntu, Debian, etc.)
- âœ… macOS (see [MACOS_FORK_FIX.md](MACOS_FORK_FIX.md) for worker setup)
- âœ… Windows (WSL recommended)

## ğŸš€ Quick Start

### 1. Clone the Repository

```bash
git clone https://github.com/Code12Git/NoCodeAiAgent.git
cd NoCodeAiAgent
```

### 2. Set Up Backend

```bash
cd workflow-builder-backend

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Create .env file
cat > .env << EOF
OPENAI_API_KEY=your_openai_api_key_here
GEMINI_API_KEY=your_gemini_api_key_here
DATABASE_URL=postgresql://username:password@localhost:5432/workflow_builder
EOF

# Start services (Qdrant and Redis)
docker run -d -p 6333:6333 qdrant/qdrant
docker run -d -p 6379:6379 redis

# Start backend
fastapi dev app/main.py
```

### 3. Set Up Frontend

```bash
# Open new terminal
cd workflow-builder-frontend

# Install dependencies
npm install

# Create .env file
echo "VITE_API_URL=http://localhost:8000" > .env

# Start frontend
npm run dev
```

### 4. Access the Application

- **Frontend**: http://localhost:5173
- **Backend API**: http://localhost:8000
- **API Docs**: http://localhost:8000/docs

## ğŸ”§ Backend Setup

### Directory Structure

```
workflow-builder-backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                    # FastAPI app
â”‚   â”œâ”€â”€ database.py                # SQLAlchemy models
â”‚   â”œâ”€â”€ api/routes/               # API endpoints
â”‚   â”œâ”€â”€ embeddings/               # Embedding services
â”‚   â”œâ”€â”€ services/                 # Business logic
â”‚   â”œâ”€â”€ vector_store/             # Qdrant integration
â”‚   â”œâ”€â”€ queue/                    # Redis queue
â”‚   â””â”€â”€ worker/                   # Background jobs
â”œâ”€â”€ data/uploads/                 # Uploaded files
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ .env                          # Environment variables
â””â”€â”€ run_worker.py                # Worker startup script
```

### Environment Variables

Create `workflow-builder-backend/.env`:

```bash
# API Keys
OPENAI_API_KEY=sk-proj-...
GEMINI_API_KEY=AIzaSy...

# Database
DATABASE_URL=postgresql://myuser:password@localhost:5432/mydb

# Qdrant (Optional - uses localhost:6333 by default)
QDRANT_URL=http://localhost:6333
# QDRANT_API_KEY=your_key_here  # For cloud Qdrant

# Redis (Optional - uses localhost:6379 by default)
REDIS_URL=redis://localhost:6379
```

### Database Setup

#### Option A: Local PostgreSQL

```bash
# Install PostgreSQL
sudo apt-get install postgresql  # Ubuntu/Debian
brew install postgresql           # macOS

# Create database
createdb workflow_builder

# Or using psql
psql -U postgres -c "CREATE DATABASE workflow_builder;"
```

#### Option B: Docker PostgreSQL

```bash
docker run -d \
  --name postgres \
  -e POSTGRES_PASSWORD=password \
  -e POSTGRES_DB=workflow_builder \
  -p 5432:5432 \
  postgres:16
```

### Vector Database Setup

#### Option A: Local Qdrant (Docker)

```bash
docker run -d \
  --name qdrant \
  -p 6333:6333 \
  -v $(pwd)/qdrant_storage:/qdrant/storage \
  qdrant/qdrant
```

#### Option B: Qdrant Cloud

1. Sign up at https://cloud.qdrant.io/
2. Create a cluster
3. Add URL and API key to `.env`

### Redis Setup

```bash
# Docker
docker run -d --name redis -p 6379:6379 redis

# Or install locally
sudo apt-get install redis-server  # Ubuntu/Debian
brew install redis                  # macOS
```

### Starting the Backend

#### Terminal 1: Backend API

```bash
cd workflow-builder-backend
source venv/bin/activate
fastapi dev app/main.py
```

#### Terminal 2: Background Worker

**Linux/Windows:**
```bash
cd workflow-builder-backend
source venv/bin/activate
python -m rq worker -c app.queue.valkey
```

**macOS:** (See [MACOS_FORK_FIX.md](MACOS_FORK_FIX.md))
```bash
cd workflow-builder-backend
source venv/bin/activate
python run_worker.py
```

### API Endpoints

- `POST /knowledge/upload` - Upload PDF document
- `POST /process/document` - Process and embed document
- `POST /llm/process` - Query with RAG
- `POST /output/follow-up` - Ask follow-up questions
- `POST /output/confidence` - Calculate confidence score
- `GET /docs` - Interactive API documentation

## ğŸ¨ Frontend Setup

### Directory Structure

```
workflow-builder-frontend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/                      # API integrations
â”‚   â”œâ”€â”€ helpers/                  # Utilities
â”‚   â”œâ”€â”€ hook/                     # Custom hooks
â”‚   â”œâ”€â”€ nodes/                    # Node components
â”‚   â”‚   â”œâ”€â”€ knowledgeBaseNode.jsx
â”‚   â”‚   â”œâ”€â”€ llmNode.jsx
â”‚   â”‚   â”œâ”€â”€ outputNode.jsx
â”‚   â”‚   â””â”€â”€ userNode.jsx
â”‚   â”œâ”€â”€ App.jsx                   # Main component
â”‚   â”œâ”€â”€ store.js                  # State management
â”‚   â”œâ”€â”€ toolbar.jsx               # Node toolbar
â”‚   â””â”€â”€ ui.jsx                    # Workflow canvas
â”œâ”€â”€ package.json
â”œâ”€â”€ vite.config.js
â””â”€â”€ .env
```

### Environment Variables

Create `workflow-builder-frontend/.env`:

```bash
VITE_API_URL=http://localhost:8000
VITE_APP_NAME=Workflow Builder
```

### Starting the Frontend

```bash
cd workflow-builder-frontend
npm install
npm run dev
```

The app will be available at http://localhost:5173

### Building for Production

```bash
npm run build
npm run preview  # Preview production build
```

## ğŸ“– Usage Guide

### Creating Your First Workflow

1. **Add User Input Node**
   - Click "User Input" in the toolbar
   - Enter your question (e.g., "What is the main topic?")

2. **Add Knowledge Base Node**
   - Click "Knowledge Base" in the toolbar
   - Upload a PDF document
   - Select embedding provider (OpenAI or Gemini)
   - Choose embedding model

3. **Add LLM Node**
   - Click "LLM" in the toolbar
   - Select LLM model (GPT-4, GPT-3.5, Gemini)
   - Adjust temperature (0.0-1.0)
   - Add custom prompt (optional)
   - Enable web search fallback (optional)

4. **Add Output Node**
   - Click "Output" in the toolbar
   - This will display the AI response

5. **Connect Nodes**
   - Drag from output handles to input handles
   - Order: User Input â†’ Knowledge Base â†’ LLM â†’ Output

6. **Submit Workflow**
   - Click "Submit Workflow" button
   - Wait for processing
   - View response in Output node

### Asking Follow-up Questions

Once you have a response:

1. Go to the Output Node
2. Enter your follow-up question in the text area
3. Click "Send Follow-up"
4. The response will update with context-aware answer

### Example Workflows

#### Simple Document Q&A

```
[User: "What is...?"] â†’ [KB: document.pdf] â†’ [LLM: GPT-4] â†’ [Output]
```

#### With Custom Prompt

```
[User: "Summarize"] â†’ [KB: doc.pdf] â†’ [LLM: GPT-4 + "Be concise"] â†’ [Output]
```

#### With Web Search Fallback

```
[User: "Latest news?"] â†’ [KB: doc.pdf] â†’ [LLM: GPT-4 + Web Search] â†’ [Output]
```

## ğŸ“š API Documentation

### Upload Document

```bash
curl -X POST http://localhost:8000/knowledge/upload \
  -F "file=@document.pdf"
```

**Response:**
```json
{
  "document_id": "unique-id.pdf",
  "chunks_count": 42,
  "chunks": [...]
}
```

### Process Document

```bash
curl -X POST http://localhost:8000/process/document \
  -H "Content-Type: application/json" \
  -d '{
    "document_id": "unique-id.pdf",
    "chunks": [...],
    "embedding_provider": "openai",
    "embedding_model": "text-embedding-3-small"
  }'
```

### Query with RAG

```bash
curl -X POST http://localhost:8000/llm/process \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What is this about?",
    "provider": "openai",
    "model": "text-embedding-3-small",
    "document_id": "unique-id.pdf",
    "llmModel": "gpt-4",
    "temperature": 0.7,
    "enable_web_search": false,
    "custom_prompt": "You are a helpful assistant."
  }'
```

**Response:**
```json
{
  "answer": "This document is about...",
  "chat_id": "conversation-uuid",
  "sources": [...],
  "confidence": 0.85
}
```

### Follow-up Question

```bash
curl -X POST http://localhost:8000/output/follow-up \
  -H "Content-Type: application/json" \
  -d '{
    "chat_id": "conversation-uuid",
    "follow_up_query": "Tell me more",
    "document_id": "unique-id.pdf",
    "llm_model": "gpt-4",
    "temperature": 0.7,
    "provider": "openai",
    "embedding_model": "text-embedding-3-small"
  }'
```

## ğŸ—‚ï¸ Project Structure

```
NoCodeAiAgent/
â”œâ”€â”€ workflow-builder-backend/       # FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/routes/            # API endpoints
â”‚   â”‚   â”œâ”€â”€ embeddings/            # Embedding providers
â”‚   â”‚   â”œâ”€â”€ services/              # Business logic
â”‚   â”‚   â”œâ”€â”€ vector_store/          # Qdrant integration
â”‚   â”‚   â”œâ”€â”€ queue/                 # Redis queue
â”‚   â”‚   â”œâ”€â”€ worker/                # Background jobs
â”‚   â”‚   â”œâ”€â”€ database.py            # SQLAlchemy models
â”‚   â”‚   â””â”€â”€ main.py                # FastAPI app
â”‚   â”œâ”€â”€ data/uploads/              # Uploaded files
â”‚   â”œâ”€â”€ requirements.txt           # Dependencies
â”‚   â”œâ”€â”€ .env                       # Environment vars
â”‚   â””â”€â”€ run_worker.py             # Worker script
â”‚
â”œâ”€â”€ workflow-builder-frontend/      # React frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ api/                   # API integrations
â”‚   â”‚   â”œâ”€â”€ nodes/                 # React Flow nodes
â”‚   â”‚   â”œâ”€â”€ App.jsx                # Main app
â”‚   â”‚   â”œâ”€â”€ store.js               # Zustand store
â”‚   â”‚   â””â”€â”€ ui.jsx                 # Workflow canvas
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ .env
â”‚
â”œâ”€â”€ MACOS_FORK_FIX.md              # macOS worker fix
â””â”€â”€ README.md                       # This file
```

## ğŸ› Troubleshooting

### Backend Issues

#### PostgreSQL Connection Error

```bash
# Check if PostgreSQL is running
sudo systemctl status postgresql  # Linux
brew services list                # macOS

# Verify credentials
psql -U username -d workflow_builder

# Update DATABASE_URL in .env
```

#### Qdrant Connection Error

```bash
# Check Qdrant status
curl http://localhost:6333/collections

# Restart Qdrant
docker restart qdrant
```

#### Worker Not Processing Jobs (macOS)

See [MACOS_FORK_FIX.md](MACOS_FORK_FIX.md) for detailed macOS worker setup.

```bash
# Use the special worker script
python run_worker.py
```

### Frontend Issues

#### Backend Connection Failed

1. Ensure backend is running on port 8000
2. Check CORS settings in `app/main.py`
3. Verify `VITE_API_URL` in `.env`

#### Nodes Not Updating

1. Clear browser cache
2. Check browser console for errors
3. Verify Zustand store updates

#### File Upload Failing

1. Check file is PDF format
2. Verify backend `/knowledge/upload` endpoint
3. Check file size limits

### Common Errors

#### "No module named 'app'"

```bash
# Ensure you're in the correct directory
cd workflow-builder-backend
source venv/bin/activate
```

#### "Cannot connect to Qdrant"

```bash
# Start Qdrant
docker run -d -p 6333:6333 qdrant/qdrant
```

#### "PostgreSQL connection refused"

```bash
# Check PostgreSQL is running
sudo service postgresql start  # Linux
brew services start postgresql # macOS
```

## ğŸ”’ Security Best Practices

- âœ… Store API keys in `.env` files
- âœ… Never commit `.env` files to Git
- âœ… Use HTTPS in production
- âœ… Implement rate limiting on API
- âœ… Validate all user inputs
- âœ… Restrict file upload types
- âœ… Use proper CORS configuration

## ğŸš€ Deployment

### Backend (Docker)

```dockerfile
FROM python:3.12-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### Frontend (Vercel/Netlify)

```bash
npm run build
# Deploy dist/ folder
```

### Environment Variables for Production

```bash
# Backend
DATABASE_URL=postgresql://prod-user:prod-pass@db-host:5432/prod-db
QDRANT_URL=https://your-qdrant-cloud.com
REDIS_URL=redis://prod-redis:6379

# Frontend
VITE_API_URL=https://api.yourapp.com
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ™ Acknowledgments

- **FastAPI** - Modern Python web framework
- **React Flow** - Node-based UI library
- **LangChain** - LLM application framework
- **Qdrant** - Vector database
- **OpenAI** - GPT models and embeddings
- **Google** - Gemini AI models

## ğŸ“ Support

- ğŸ› **Issues**: [GitHub Issues](https://github.com/Code12Git/NoCodeAiAgent/issues)
- ğŸ“š **Docs**: http://localhost:8000/docs (when running)
- ğŸ’¬ **Discussions**: [GitHub Discussions](https://github.com/Code12Git/NoCodeAiAgent/discussions)

---

**Built with â¤ï¸ by Code12Git**

**Star â­ this repo if you find it useful!**
